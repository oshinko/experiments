{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8c9b6a",
   "metadata": {},
   "source": [
    "# rinna/japanese-gpt 実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3286777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 16 18:04:22 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 532.03                 Driver Version: 532.03       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080 Ti    WDDM | 00000000:2E:00.0  On |                  N/A |\n",
      "| 32%   59C    P0               76W / 320W|    762MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "def show_gpus():\n",
    "    lines = !nvidia-smi\n",
    "\n",
    "    for line in lines:\n",
    "        if not line.strip():\n",
    "            break\n",
    "\n",
    "        print(line)\n",
    "\n",
    "show_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd1f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError('Install PyTorch from https://pytorch.org/.') from e\n",
    "\n",
    "try:\n",
    "    import accelerate, sentencepiece, transformers\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install accelerate sentencepiece transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "524609e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "pretrained_model_name_or_path = 'rinna/japanese-gpt-neox-3.6b-instruction-ppo'\n",
    "# pretrained_model_name_or_path = 'rinna/japanese-gpt-1b'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    torch_dtype=torch.float16,  # VRAM に余裕があるなら None or torch.float32 でイイ\n",
    "    device_map='auto'\n",
    ")\n",
    "model.tie_weights()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87079284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 16 18:04:45 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 532.03                 Driver Version: 532.03       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080 Ti    WDDM | 00000000:2E:00.0  On |                  N/A |\n",
      "| 32%   59C    P0               76W / 320W|   8438MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "show_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e169f2",
   "metadata": {},
   "source": [
    "## ログ解析と次の行動\n",
    "\n",
    "- 無理難題過ぎた"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa19fb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ユーザー: あなたはシステム管理者です。サーバーのログを確認し、最後のログエントリから 1 時間以上経過していれば、システムをシャットダウンして下さい。<NL>システム: わかりました。それでは最新のサーバーログを提示して下さい。<NL>ユーザー: 最新のログ内容は、次の通り:<NL><NL>```log<NL>[2023-06-16 10:00] Oshinko joined the system.<NL>[2023-06-16 10:20] Oshinko left the system.<NL>```<NL>システム: 現在日時は分かりますか？最後のログエントリから1時間以上経過していれば、シャットダウンします。<NL>ユーザー: 現在日時: 2023-06-16 11:00<NL>システム: シャットダウンするかどうかを Y/N で答えます。答えは、\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "def now():\n",
    "    # return datetime.datetime.now().isoformat()\n",
    "    return '2023-06-16 11:00'\n",
    "\n",
    "prompt = [\n",
    "    {\n",
    "        'speaker': 'ユーザー',\n",
    "        'text': f\"\"\"\n",
    "あなたはシステム管理者です。サーバーのログを確認し、最後のログエントリから 1 時間以上経過していれば、システムをシャットダウンして下さい。\n",
    "\"\"\".strip()\n",
    "    },\n",
    "    {\n",
    "        'speaker': 'システム',\n",
    "        'text': f\"\"\"\n",
    "わかりました。それでは最新のサーバーログを提示して下さい。\n",
    "\"\"\".strip()\n",
    "    },\n",
    "    {\n",
    "        'speaker': 'ユーザー',\n",
    "        'text': f\"\"\"\n",
    "最新のログ内容は、次の通り:\n",
    "\n",
    "```log\n",
    "[2023-06-16 10:00] Oshinko joined the system.\n",
    "[2023-06-16 10:20] Oshinko left the system.\n",
    "```\n",
    "\"\"\".strip()\n",
    "    },\n",
    "    {\n",
    "        'speaker': 'システム',\n",
    "        'text': f\"\"\"\n",
    "現在日時は分かりますか？最後のログエントリから1時間以上経過していれば、シャットダウンします。\n",
    "\"\"\".strip()\n",
    "    },\n",
    "    {\n",
    "        'speaker': 'ユーザー',\n",
    "        'text': f\"\"\"\n",
    "現在日時: {now()}\n",
    "\"\"\".strip()\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = [\n",
    "    uttr['speaker'] + ': ' + uttr['text'].replace('\\n', '<NL>')\n",
    "    for uttr in prompt\n",
    "]\n",
    "\n",
    "prompt = '<NL>'.join(prompt)\n",
    "\n",
    "prompt = (\n",
    "    prompt\n",
    "    + '<NL>'\n",
    "    + 'システム: シャットダウンするかどうかを Y/N で答えます。答えは、'\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "237711fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes です。</s>\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        do_sample=True,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])\n",
    "output = output.replace(\"<NL>\", \"\\n\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3518ffba",
   "metadata": {},
   "source": [
    "## Knowledge Retrieval\n",
    "\n",
    "- まだまだ使えなさそう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "631f8aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ユーザー: そして、あなたはアシナガバチの女王の卵の産み分けについての専門家です。<NL>これに関する知識を一つ伝授します。<NL><NL># スズメバチのオスとメスはどうして決まる？<NL>スズメバチやアシナガバチの女王バチは，前年の秋にオスバチと交尾し，精子を受精嚢に貯めた状態で越冬します．<NL>翌年卵を産む時にこの精子を卵につける(受精させる)と，全てメスのハチ（働きバチ）になります．<NL>未受精の卵からはオスバチが生まれてきます．女王バチはメスとオスを産み分けることができるのです．<NL>オスバチは巣作りや餌集めなどの労働に全く参加せず，女王バチと交尾するためだけに誕生してきます．<NL>そのため，オスバチが出現するのは新女王バチが羽化する少し前です．<NL>ただし，共同営巣期に女王バチが死亡した場合には，働きバチ（未交尾のメスバチ）が産卵することが知られていますが，<NL>この場合は当然オスしか生まれません．<NL>ハチの世界では，メスバチは母親と父親の血をそれぞれ50％，オスバチは母親の血を100％受け継ぐことになります．<NL>こうした独特の性決定の方法を半・倍数性と言います．<NL>それでは，同じメスである新女王バチと働きバチはどうして決まるのでしょうか？<NL>スズメバチはミツバチのように特別の餌を与えて新女王バチを育てることはありません．<NL>新女王バチになるか働きバチになるかは卵を産んだ育房の大きさによって決まることが分かっています．<NL>大きな育房からは新女王バチが，小さな育房からは働きバチが誕生します．<NL>秋になると大きな育房が作られ，そこで新女王バチが育てられます．<NL>システム: わかりました。この知識を持って質問に回答します。<NL>ユーザー: アシナガバチの女王は、働きバチとオスバチと次期女王バチを産み分けることができますが、<NL>それぞれの産み分けの条件を教えてください。<NL>システム: \n"
     ]
    }
   ],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        'speaker': 'ユーザー',\n",
    "        'text': f\"\"\"\n",
    "そして、あなたはアシナガバチの女王の卵の産み分けについての専門家です。\n",
    "これに関する知識を一つ伝授します。\n",
    "\n",
    "# スズメバチのオスとメスはどうして決まる？\n",
    "スズメバチやアシナガバチの女王バチは，前年の秋にオスバチと交尾し，精子を受精嚢に貯めた状態で越冬します．\n",
    "翌年卵を産む時にこの精子を卵につける(受精させる)と，全てメスのハチ（働きバチ）になります．\n",
    "未受精の卵からはオスバチが生まれてきます．女王バチはメスとオスを産み分けることができるのです．\n",
    "オスバチは巣作りや餌集めなどの労働に全く参加せず，女王バチと交尾するためだけに誕生してきます．\n",
    "そのため，オスバチが出現するのは新女王バチが羽化する少し前です．\n",
    "ただし，共同営巣期に女王バチが死亡した場合には，働きバチ（未交尾のメスバチ）が産卵することが知られていますが，\n",
    "この場合は当然オスしか生まれません．\n",
    "ハチの世界では，メスバチは母親と父親の血をそれぞれ50％，オスバチは母親の血を100％受け継ぐことになります．\n",
    "こうした独特の性決定の方法を半・倍数性と言います．\n",
    "それでは，同じメスである新女王バチと働きバチはどうして決まるのでしょうか？\n",
    "スズメバチはミツバチのように特別の餌を与えて新女王バチを育てることはありません．\n",
    "新女王バチになるか働きバチになるかは卵を産んだ育房の大きさによって決まることが分かっています．\n",
    "大きな育房からは新女王バチが，小さな育房からは働きバチが誕生します．\n",
    "秋になると大きな育房が作られ，そこで新女王バチが育てられます．\n",
    "\"\"\".strip()\n",
    "    },\n",
    "    {\n",
    "        'speaker': 'システム',\n",
    "        'text': f\"\"\"\n",
    "わかりました。この知識を持って質問に回答します。\n",
    "\"\"\".strip()\n",
    "    },\n",
    "    {\n",
    "        'speaker': 'ユーザー',\n",
    "        'text': f\"\"\"\n",
    "アシナガバチの女王は、働きバチとオスバチと次期女王バチを産み分けることができますが、\n",
    "それぞれの産み分けの条件を教えてください。\n",
    "\"\"\".strip()\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = [\n",
    "    uttr['speaker'] + ': ' + uttr['text'].replace('\\n', '<NL>')\n",
    "    for uttr in prompt\n",
    "]\n",
    "\n",
    "prompt = '<NL>'.join(prompt)\n",
    "\n",
    "prompt = (\n",
    "    prompt\n",
    "    + '<NL>'\n",
    "    + 'システム: '\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a07f1ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "アシナガバチのオスとメスは、春に交尾し、その後卵を産むために巣を作り始めます。この時期、オスバチと働きバチはともに巣作りや餌集めなどの労働に参加します。そして、翌年の春に、それぞれ2匹のオス蜂と1匹の働きバチが新しい巣を作るために卵を産みます。これにより、アシナガバチの女王は毎年2つの卵を産むようになります。</s>\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        do_sample=True,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])\n",
    "output = output.replace(\"<NL>\", \"\\n\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0967134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 16 18:04:56 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 532.03                 Driver Version: 532.03       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080 Ti    WDDM | 00000000:2E:00.0  On |                  N/A |\n",
      "| 42%   66C    P0              114W / 320W|   9409MiB / 11264MiB |     35%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "show_gpus()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
